{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and training a simple neural network (MLP)\n",
    "===================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to configure the GPU\n",
    "!pip install glasspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create and train a simple multilayer perceptron (MLP) neural network. We will use the `lightning` module to do this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glasspy.data import SciGlass\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to collect, process and split the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_prop = {\n",
    "    \"must_have_and\": [\"Tg\"],\n",
    "}\n",
    "\n",
    "config_comp = {\n",
    "    \"must_have_only\": [\n",
    "        \"SiO2\",\n",
    "        \"Li2O\",\n",
    "        \"Na2O\",\n",
    "        \"K2O\",\n",
    "        \"CaO\",\n",
    "        \"MgO\",\n",
    "        \"BaO\",\n",
    "        \"Al2O3\",\n",
    "        \"TiO2\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "source = SciGlass(\n",
    "    elements_cfg={},\n",
    "    properties_cfg=config_prop,\n",
    "    compounds_cfg=config_comp,\n",
    ")\n",
    "\n",
    "source.remove_duplicate_composition(\n",
    "    scope=\"compounds\",\n",
    "    decimals=3,\n",
    "    aggregator=\"median\",\n",
    ")\n",
    "\n",
    "df = source.data\n",
    "\n",
    "df[\"property\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.index\n",
    "\n",
    "X = df[\"compounds\"]\n",
    "y = df[\"property\"][\"Tg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on why we need to split the data into training, test and validation datasets, see **Raschka, Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, (2020). [https://doi.org/10.48550/arXiv.1811.12808](https://doi.org/10.48550/arXiv.1811.12808)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "RANDOM_SEED = 61455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df.index\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    indices, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train_val = X.loc[train_val_idx]\n",
    "y_train_val = y.loc[train_val_idx]\n",
    "\n",
    "X_test = X.loc[test_idx].values\n",
    "y_test = y.loc[test_idx].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train = X.loc[train_idx].values\n",
    "y_train = y.loc[train_idx].values.reshape(-1,1)\n",
    "\n",
    "X_val = X.loc[val_idx].values\n",
    "y_val = y.loc[val_idx].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data is usually a good practice when training neural networks. Note that to avoid *data leakage*, we can only train the scaler with the training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MaxAbsScaler()\n",
    "x_scaler.fit(X_train)\n",
    "\n",
    "y_scaler = MaxAbsScaler()\n",
    "y_scaler.fit(y_train)\n",
    "\n",
    "X_train = x_scaler.transform(X_train)\n",
    "y_train = y_scaler.transform(y_train)\n",
    "\n",
    "X_val = x_scaler.transform(X_val)\n",
    "y_val = y_scaler.transform(y_val)\n",
    "\n",
    "X_test = x_scaler.transform(X_test)\n",
    "y_test = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataModule class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataModule` class takes care of feeding data to the neural network model during training and validation. While you can write your own `DataModule` class, the code below should be good for most use cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        batch_size = 256,\n",
    "        num_workers = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            TensorDataset(self.X_train, self.y_train),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            TensorDataset(self.X_val, self.y_val),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            TensorDataset(self.X_test, self.y_test),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to instantiate the `DataModule` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are complex machine learning algorithms with many dials and knobs to configure. When using `Pytorch` or `Lightning`, we need to create the neural network class. Below is an example that creates the `MLP` class with two hidden layers. The methods ending in `_step` are necessary for training and you probably won&rsquo;t want to change them when you start learning neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, num_features, layer1, layer2, num_targets\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, layer1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(layer1, layer2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(layer2, num_targets),\n",
    "        )\n",
    "\n",
    "        self.loss_fun = F.mse_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fun(y, y_pred)\n",
    "\n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fun(y, y_pred)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fun(y, y_pred)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to instantiate the class we have just created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.shape[1]\n",
    "num_targets = 1\n",
    "layer1 = 3\n",
    "layer2 = 2\n",
    "\n",
    "my_mlp = MLP(\n",
    "    num_features, layer1, layer2, num_targets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that an epoch has passed when the neural network &ldquo;sees&rdquo; all the training data. Training a good neural network usually involves running the data through it many times. You can control this with the `max_epochs` argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "trainer = L.Trainer(max_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(my_mlp, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will put the network into `evaluation` mode and test if it is a reasonable model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"test\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_true = dm.X_test\n",
    "\n",
    "    y_true = dm.y_test\n",
    "    y_true = y_scaler.inverse_transform(y_true)\n",
    "\n",
    "    y_pred = my_mlp(X_true)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "    RMSE = mean_squared_error(y_true, y_pred) ** (1/2)\n",
    "\n",
    "    print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not trained this neural network in a deterministic way. Therefore, every time you run this code, you will (most likely) get a different result. Nevertheless, we have created a very, very, very simple neural network and it is unreasonable to think that it will perform well for the glass transition data we have collected. Try increasing the number of neurons to see if the performance improves!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wish to save your model for future use. This is easy to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"my_model.p\"\n",
    "torch.save(my_mlp.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can load it&#x2026;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_mlp = MLP(\n",
    "    num_features, layer1, layer2, num_targets\n",
    ")\n",
    "\n",
    "state_dict = torch.load(file_name, weights_only=True)\n",
    "other_mlp.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x2026; and check that it gives the same results (as it should).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_mlp.eval()\n",
    "\n",
    "dm.setup(\"test\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_true = dm.X_test\n",
    "\n",
    "    y_true = dm.y_test\n",
    "    y_true = y_scaler.inverse_transform(y_true)\n",
    "\n",
    "    y_pred = other_mlp(X_true)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "    RMSE = mean_squared_error(y_true, y_pred) ** (1/2)\n",
    "\n",
    "    print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the GPU greatly increases the training speed. Make sure you have the GPU-enabled version of `Pytorch` and updated GPU drivers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yet_another_mlp = MLP(num_features, layer1, layer2, num_targets)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=NUM_EPOCHS,\n",
    ")\n",
    "\n",
    "trainer.fit(yet_another_mlp, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
